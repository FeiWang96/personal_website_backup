<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Fei  Wang | Publications</title>
    <meta name="author" content="Fei  Wang" />
    <meta name="description" content="<em>*</em> denotes equal contribution and joint lead authorship." />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=EB+Garamond:100,300,400,500,600,700,800|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒŒ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://feiwang96.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://FeiWang96.github.io/"><span class="font-weight-bold">Fei</span>   Wang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"><em>*</em> denotes equal contribution and joint lead authorship.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">preprint</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="liu2023shortcuts" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2305.14910" target="_blank" rel="noopener noreferrer"><b>From Shortcuts to Triggers: Backdoor Defense with Denoised PoE</b></a></div>
          
          <!-- Author -->
          <div class="author">Qin Liu,Â 
                  <em>Fei Wang</em>,Â Chaowei Xiao,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
<!-- -->
          
          
          
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">preprint</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="xu2023instructions" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2305.14710" target="_blank" rel="noopener noreferrer"><b>Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</b></a></div>
          
          <!-- Author -->
          <div class="author">Jiashu Xu,Â Mingyu Derek Ma,Â 
                  <em>Fei Wang</em>,Â Chaowei Xiao,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
<!-- -->
          
          
          
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">EMNLP</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2023causal" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2305.14695" target="_blank" rel="noopener noreferrer"><b>A Causal View of Entity Bias in (Large) Language Models</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang</em>,Â Wenjie Mo,Â Yiwei Wang,Â Wenxuan Zhou,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Findings of EMNLP</em> 2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
<!--
            <a href="https://github.com/luka-group/Causal-View-of-Entity-Bias" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/Causal-View-of-Entity-Bias" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">EMNLP</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="nan2023dense" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2310.18619" target="_blank" rel="noopener noreferrer"><b>Dense Retrieval as Indirect Supervision for Large-space Decision Making</b></a></div>
          
          <!-- Author -->
          <div class="author">Nan Xu,Â 
                  <em>Fei Wang</em>,Â Mingtao Dong,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Findings of EMNLP</em> 2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
<!--
            <a href="https://github.com/luka-group/DDR" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/DDR" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">CoNLL</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2023entred" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2305.13551" target="_blank" rel="noopener noreferrer"><b>How Fragile is Relation Extraction under Entity Replacements?</b></a></div>
          
          <!-- Author -->
          <div class="author">Yiwei Wang,Â Bryan Hooi,Â 
                  <em>Fei Wang</em>,Â Yujun Cai,Â Yuxuan Liang,Â Wenxuan Zhou,Â Jing Tang,Â Manjuan Duan,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of CoNLL</em> 2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
<!--
            <a href="https://github.com/wangywUST/ENTRED" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/wangywUST/ENTRED" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">AACL</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2023self" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2309.10891" target="_blank" rel="noopener noreferrer"><b>Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang</em>,Â Kuan-Hao Huang,Â Kai-Wei Chang,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of AACL</em> 2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/luka-group/SALT" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/SALT" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">ACL</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2023robust" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2305.17627" target="_blank" rel="noopener noreferrer"><b>Robust Natural Language Understanding with Residual Attention Debiasing</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang*</em>,Â James Y. Huang*,Â Tianyi Yan,Â Wenxuan Zhou,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Findings of ACL</em> 2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/luka-group/READ" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/READ" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial role in providing robust prediction. In this paper, we propose REsidual Attention Debiasing (READ), an end-to-end debiasing method that mitigates unintended biases from attention. Experiments on three NLU tasks show that READ significantly improves the performance of BERT-based models on OOD data with shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the crucial role of unbiased attention in robust NLU models and that READ effectively mitigates biases in attention.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">ACL</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="dixit2023improving" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2305.14981" target="_blank" rel="noopener noreferrer"><b>Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality</b></a></div>
          
          <!-- Author -->
          <div class="author">Tanay Dixit,Â 
                  <em>Fei Wang</em>,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of ACL</em> 2023
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/tanay2001/EFactSum" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/tanay2001/EFactSum" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose EFACTSUM (i.e., Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing summary quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">EMNLP</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2022salience" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2210.12330" target="_blank" rel="noopener noreferrer"><b>Salience Allocation as Guidance for Abstractive Summarization</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang*</em>,Â Kaiqiang Song*,Â Hongming Zhang,Â Lifeng Jin,Â Sangwoo Cho,Â Wenlin Yao,Â Xiaoyang Wang,Â <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>,Â and Dong Yu
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of EMNLP</em> 2022
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/tencent-ailab/season" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/tencent-ailab/season" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Abstractive summarization models typically learn to capture the salient information from scratch implicitly. Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. Furthermore, it cannot easily adapt to documents with various abstractiveness. As the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">EMNLP</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="xu2022does" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2205.12640" target="_blank" rel="noopener noreferrer"><b>Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing</b></a></div>
          
          <!-- Author -->
          <div class="author">Nan Xu,Â 
                  <em>Fei Wang</em>,Â Bangzheng Li,Â Mingtao Dong,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of EMNLP</em> 2022
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/luka-group/DiagnoseET" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/DiagnoseET" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The entity typing task aims at predicting one or more words or phrases that describe the type(s) of a specific mention in a sentence. Due to shortcuts from surface patterns to annotated entity labels and biased training, existing entity typing models are subject to the problem of spurious correlations. To comprehensively investigate the faithfulness and reliability of entity typing methods, we first systematically define distinct kinds of model biases that are reflected mainly from spurious correlations. Particularly, we identify six types of existing model biases, including mention-context bias, lexical overlapping bias, named entity bias, pronoun bias, dependency bias, and overgeneralization bias. To mitigate these model biases, we then introduce a counterfactual data augmentation method. By augmenting the original training set with their bias-free counterparts, models are forced to fully comprehend the sentences and discover the fundamental cues for entity typing, rather than relying on spurious correlations for shortcuts. Experimental results on the UFET dataset show that our counterfactual data augmentation approach helps improve generalization of different entity typing models with consistently better performance on both in- and out-of-distribution test sets.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">NAACL</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2022robust" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2205.03972" target="_blank" rel="noopener noreferrer"><b>Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang</em>,Â Zhewei Xu,Â Pedro Szekely,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of NAACL</em> 2022
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/luka-group/Lattice" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/Lattice" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">EMNLP MMNLU</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2022zero" class="col-sm-8">
        
          <!-- Title -->
          
            
              <div class="title"><b>Zero-shot cross-lingual sequence tagging as Seq2Seq generation for joint intent classification and slot filling</b></div>
            
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang</em>,Â Kuan-Hao Huang,Â Anoop Kumar,Â Aram Galstyan,Â Greg Ver Steeg,Â and Kai-Wei Chang
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of the MMNLU Workshop at EMNLP</em> 2022
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!-- -->
          
          
          
            <a href="https://aclanthology.org/2022.mmnlu-1.6/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The joint intent classification and slot filling task seeks to detect the intent of an utterance and extract its semantic concepts. In the zero-shot cross-lingual setting, a model is trained on a source language and then transferred to other target languages through multi-lingual representations without additional training data. While prior studies show that pre-trained multilingual sequence-to-sequence (Seq2Seq) models can facilitate zero-shot transfer, there is little understanding on how to design the output template for the joint prediction tasks. In this paper, we examine three aspects of the output template â€“ (1) label mapping, (2) task dependency, and (3) word order. Experiments on the MASSIVE dataset consisting of 51 languages show that our output template significantly improves the performance of pre-trained cross-lingual language models.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">EMNLP</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2021table" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2109.04053" target="_blank" rel="noopener noreferrer"><b>Table-based Fact Verification With Salience-aware Learning</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang</em>,Â Kexuan Sun,Â Jay Pujara,Â Pedro Szekely,Â and <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Findings of EMNLP</em> 2021
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/luka-group/Salience-aware-Learning" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/luka-group/Salience-aware-Learning" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Tables provide valuable knowledge that can be used to verify textual statements. While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available. Moreover, training a generalized fact verification model requires abundant labeled training data. In this paper, we propose a novel system to address these problems. Inspired by counterfactual causality, our system identifies token-level salience in the statement with probing-based salience estimation. Salience estimation allows enhanced learning of fact verification from two perspectives. From one perspective, our system conducts masked salient token prediction to enhance the model for alignment and reasoning between the table and the statement. From the other perspective, our system applies salience-aware data augmentation to generate a more diverse set of training instances by replacing non-salient terms. Experimental results on TabFact show the effective improvement by the proposed salience-aware learning techniques, leading to the new SOTA performance on the benchmark.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">CIKM</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="sun2021tabular" class="col-sm-8">
        
          <!-- Title -->
          
            
              <div class="title"><a href="https://dl.acm.org/doi/10.1145/3459637.3482484" target="_blank" rel="noopener noreferrer"><b>Tabular Functional Block Detection with Embedding-based Agglomerative Cell Clustering</b></a></div>
            
          
          <!-- Author -->
          <div class="author">Kexuan Sun,Â 
                  <em>Fei Wang</em>,Â <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>,Â and Jay Pujara
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of CIKM</em> 2021
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!-- -->
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Tables are a widely-used format for data curation. The diversity of domains, layouts, and content of tables makes knowledge extraction challenging. Understanding table layouts is an important step for automatically harvesting knowledge from tabular data. Since table cells are spatially organized into regions, correctly identifying such regions and inferring their functional roles, referred to as functional block detection, is a critical part of understanding table layouts. Earlier functional block detection approaches fail to leverage spatial relationships and higher-level structure, either depending on cell-level predictions or relying on data types as signals for identifying blocks. In this paper, we introduce a flexible functional block detection method by applying agglomerative clustering techniques which merge smaller blocks into larger blocks using two merging strategies. Our proposed method uses cell embeddings with a customized dissimilarity function which utilizes local and margin distances, as well as block coherence metrics to capture cell, block, and table scoped features. Given the diversity of tables in real-world corpora, we also introduce a sampling-based approach for automatically tuning distance thresholds for each table. Experimental results show that our method improves over the earlier state-of-the-art method in terms of several evaluation metrics.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">SIGIR</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wang2021retrieving" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/2105.01736" target="_blank" rel="noopener noreferrer"><b>Retrieving Complex Tables with Multi-Granular Graph Representation Learning</b></a></div>
          
          <!-- Author -->
          <div class="author">
                  <em>Fei Wang</em>,Â Kexuan Sun,Â <a href="https://muhaochen.github.io/" target="_blank" rel="noopener noreferrer">Muhao Chen</a>,Â Jay Pujara,Â and Pedro Szekely
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of SIGIR</em> 2021
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/FeiWang96/GTR" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/FeiWang96/GTR" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The task of natural language table retrieval (NLTR) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the NLTR problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">ACL</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="wu2020corefqa" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/1911.01746" target="_blank" rel="noopener noreferrer"><b>CorefQA: Coreference resolution as query-based span prediction</b></a></div>
          
          <!-- Author -->
          <div class="author">Wei Wu,Â 
                  <em>Fei Wang</em>,Â Arianna Yuan,Â Fei Wu,Â and <a href="https://nlp.stanford.edu/~bdlijiwei/" target="_blank" rel="noopener noreferrer">Jiwei Li</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of ACL</em> 2020
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!--
            <a href="https://github.com/ShannonAI/CorefQA" class="btn btn-sm z-depth-0" role="button">Code</a> -->
          
            <abbr class="badge repo"><a href="https://github.com/ShannonAI/CorefQA" target="_blank" rel="noopener noreferrer">Code</a></abbr>
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper, we present an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in machine reading comprehension (MRC): A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the MRC framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing MRC datasets can be used for data augmentation to improve the modelâ€™s generalization capability. Experiments demonstrate significant performance boost over previous models, with 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">NeurIPS</abbr><!--
            
          -->
</div>

        <!-- Entry bib key -->
        <div id="meng2019glyce" class="col-sm-8">
        
          <!-- Title -->
          
            <div class="title"><a href="http://arxiv.org/abs/1901.10125" target="_blank" rel="noopener noreferrer"><b>Glyce: Glyph-vectors for chinese character representations</b></a></div>
          
          <!-- Author -->
          <div class="author">Yuxian Meng*,Â Wei Wu*,Â 
                  <em>Fei Wang*</em>,Â Xiaoya Li*,Â Ping Nie,Â Fan Yin,Â Muyu Li,Â Qinghong Han,Â Xiaofei Sun,Â and <a href="https://nlp.stanford.edu/~bdlijiwei/" target="_blank" rel="noopener noreferrer">Jiwei Li</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          
            <div class="periodical">
              <em>In Proceedings of NeurIPS</em> 2019
            </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          
            <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
          
<!-- -->
          
          
          
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the modelâ€™s ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8% on the Fudan corpus for text classification.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2023 Fei  Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-221464197-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-221464197-1');
  </script>
  </body>
</html>

