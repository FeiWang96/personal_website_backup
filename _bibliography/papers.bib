---
---
@misc{wang2023causal,
      title={A Causal View of Entity Bias in (Large) Language Models}, 
      author={Fei Wang and Wenjie Mo and Yiwei Wang and Wenxuan Zhou and Muhao Chen},
      year={2023},
      arxiv={2305.14695},
      booktitle={Findings of EMNLP},
      abbr={EMNLP},
      selected={true}
}

@misc{liu2023shortcuts,
      title={From Shortcuts to Triggers: Backdoor Defense with Denoised PoE}, 
      author={Qin Liu and Fei Wang and Chaowei Xiao and Muhao Chen},
      year={2023},
      arxiv={2305.14910},
      booktitle={arXiv preprint},
      abbr={preprint},
      selected={false}
}

@misc{xu2023instructions,
      title={Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models}, 
      author={Jiashu Xu and Mingyu Derek Ma and Fei Wang and Chaowei Xiao and Muhao Chen},
      year={2023},
      arxiv={2305.14710},
      booktitle={arXiv preprint},
      abbr={preprint},
      selected={false}
}

@misc{wang2023entred,
      title={EntRED: Benchmarking Relation Extraction with Fewer Shortcuts}, 
      author={Yiwei Wang and Bryan Hooi and Fei Wang and Yujun Cai and Yuxuan Liang and Wenxuan Zhou and Jing Tang and Manjuan Duan and Muhao Chen},
      year={2023},
      arxiv={2305.13551},
      booktitle={arXiv preprint},
      abbr={preprint},
      selected={false},
      code={https://github.com/wangywUST/ENTRED}
}

@inproceedings{wang2023self,
  title={Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer},
  author={Wang, Fei and Huang, Kuan-Hao and Chang, Kai-Wei and Chen, Muhao},
  arXiv={2309.10891},
  year={2023},
  booktitle={Proceedings of AACL},
  abbr={AACL},
  selected={false},
  code={https://github.com/luka-group/SALT},
  abstract={Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. }
}

@inproceedings{wang2023robust,
  title={Robust Natural Language Understanding with Residual Attention Debiasing},
  author={Wang*, Fei and Huang*, James Y. and Yan, Tianyi and Zhou, Wenxuan and Chen, Muhao},
  year={2023},
  arxiv={2305.17627},
  booktitle={Findings of ACL},
  abbr={ACL},
  selected={true},
  code={https://github.com/luka-group/READ},
  abstract={Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial role in providing robust prediction. In this paper, we propose REsidual Attention Debiasing (READ), an end-to-end debiasing method that mitigates unintended biases from attention. Experiments on three NLU tasks show that READ significantly improves the performance of BERT-based models on OOD data with shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the crucial role of unbiased attention in robust NLU models and that READ effectively mitigates biases in attention.}
}

@inproceedings{dixit2023improving,
  title={Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality},
  author={Dixit, Tanay and Wang, Fei and Chen, Muhao},
  year={2023},
  booktitle={Proceedings of ACL},
  abbr={ACL},
  selected={false},
  arxiv={2305.14981},
  code={https://github.com/tanay2001/EFactSum},
  abstract={Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose EFACTSUM (i.e., Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing summary quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.}
}

@inproceedings{wang2022salience,
  title={Salience Allocation as Guidance for Abstractive Summarization},
  author={Wang*, Fei and Song*, Kaiqiang and Zhang, Hongming and Jin, Lifeng and Cho, Sangwoo and Yao, Wenlin and Wang, Xiaoyang and Chen, Muhao and Yu,Dong},
  year={2022},
  booktitle={Proceedings of EMNLP},
  abbr={EMNLP},
  selected={true},
  arxiv={2210.12330},
  code={https://github.com/tencent-ailab/season},
  abstract={Abstractive summarization models typically learn to capture the salient information from scratch implicitly. Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. Furthermore, it cannot easily adapt to documents with various abstractiveness. As the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles.}
}

@inproceedings{xu2022does,
  title={Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing},
  author={Xu, Nan and Wang, Fei and Li, Bangzheng and Dong, Mingtao and Chen, Muhao},
  year={2022},
  booktitle={Proceedings of EMNLP},
  abbr={EMNLP},
  arxiv={2205.12640},
  code={https://github.com/luka-group/DiagnoseET},
  abstract={The entity typing task aims at predicting one or more words or phrases that describe the type(s) of a specific mention in a sentence. Due to shortcuts from surface patterns to annotated entity labels and biased training, existing entity typing models are subject to the problem of spurious correlations. To comprehensively investigate the faithfulness and reliability of entity typing methods, we first systematically define distinct kinds of model biases that are reflected mainly from spurious correlations. Particularly, we identify six types of existing model biases, including mention-context bias, lexical overlapping bias, named entity bias, pronoun bias, dependency bias, and overgeneralization bias. To mitigate these model biases, we then introduce a counterfactual data augmentation method. By augmenting the original training set with their bias-free counterparts, models are forced to fully comprehend the sentences and discover the fundamental cues for entity typing, rather than relying on spurious correlations for shortcuts. Experimental results on the UFET dataset show that our counterfactual data augmentation approach helps improve generalization of different entity typing models with consistently better performance on both in- and out-of-distribution test sets.}
}

@inproceedings{wang2022robust,
  title={Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning},
  author={Wang, Fei and Xu, Zhewei and Szekely, Pedro and Chen, Muhao},
  year={2022},
  booktitle={Proceedings of NAACL},
  abbr={NAACL},
  code={https://github.com/luka-group/Lattice},
  selected={true},
  arxiv={2205.03972},
  abstract={Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.}
}

@inproceedings{wang2022zero,
  title={Zero-shot cross-lingual sequence tagging as Seq2Seq generation for joint intent classification and slot filling},
  author={Wang, Fei and Huang, Kuan-Hao and Kumar, Anoop and Galstyan, Aram and Ver Steeg, Greg and Chang, Kai-Wei},
  booktitle={Proceedings of the MMNLU Workshop at EMNLP},
  year={2022},
  abbr={EMNLP MMNLU},
  html={https://aclanthology.org/2022.mmnlu-1.6/},
  abstract={The joint intent classification and slot filling task seeks to detect the intent of an utterance and extract its semantic concepts. In the zero-shot cross-lingual setting, a model is trained on a source language and then transferred to other target languages through multi-lingual representations without additional training data. While prior studies show that pre-trained multilingual sequence-to-sequence (Seq2Seq) models can facilitate zero-shot transfer, there is little understanding on how to design the output template for the joint prediction tasks. In this paper, we examine three aspects of the output template â€“ (1) label mapping, (2) task dependency, and (3) word order. Experiments on the MASSIVE dataset consisting of 51 languages show that our output template significantly improves the performance of pre-trained cross-lingual language models.}
}

@inproceedings{wang2021table,
  title={Table-based Fact Verification With Salience-aware Learning},
  author={Wang, Fei and Sun, Kexuan and Pujara, Jay and Szekely, Pedro and Chen, Muhao},
  year={2021},
  booktitle={Findings of EMNLP},
  abbr={EMNLP},
  code={https://github.com/luka-group/Salience-aware-Learning},
  arxiv={2109.04053},
  abstract={Tables provide valuable knowledge that can be used to verify textual statements. While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available. Moreover, training a generalized fact verification model requires abundant labeled training data. In this paper, we propose a novel system to address these problems. Inspired by counterfactual causality, our system identifies token-level salience in the statement with probing-based salience estimation. Salience estimation allows enhanced learning of fact verification from two perspectives. From one perspective, our system conducts masked salient token prediction to enhance the model for alignment and reasoning between the table and the statement. From the other perspective, our system applies salience-aware data augmentation to generate a more diverse set of training instances by replacing non-salient terms. Experimental results on TabFact show the effective improvement by the proposed salience-aware learning techniques, leading to the new SOTA performance on the benchmark.}
}

@inproceedings{sun2021tabular,
  title={Tabular Functional Block Detection with Embedding-based Agglomerative Cell Clustering},
  author={Sun, Kexuan and Wang, Fei and Chen, Muhao and Pujara, Jay},
  year={2021},
  booktitle={Proceedings of CIKM},
  abbr={CIKM},
  url={https://dl.acm.org/doi/10.1145/3459637.3482484},
  abstract={Tables are a widely-used format for data curation. The diversity of domains, layouts, and content of tables makes knowledge extraction challenging. Understanding table layouts is an important step for automatically harvesting knowledge from tabular data. Since table cells are spatially organized into regions, correctly identifying such regions and inferring their functional roles, referred to as functional block detection, is a critical part of understanding table layouts. Earlier functional block detection approaches fail to leverage spatial relationships and higher-level structure, either depending on cell-level predictions or relying on data types as signals for identifying blocks. In this paper, we introduce a flexible functional block detection method by applying agglomerative clustering techniques which merge smaller blocks into larger blocks using two merging strategies. Our proposed method uses cell embeddings with a customized dissimilarity function which utilizes local and margin distances, as well as block coherence metrics to capture cell, block, and table scoped features. Given the diversity of tables in real-world corpora, we also introduce a sampling-based approach for automatically tuning distance thresholds for each table. Experimental results show that our method improves over the earlier state-of-the-art method in terms of several evaluation metrics.}
}

@inproceedings{wang2021retrieving,
  title={Retrieving Complex Tables with Multi-Granular Graph Representation Learning},
  author={Wang, Fei and Sun, Kexuan and Chen, Muhao and Pujara, Jay and Szekely, Pedro},
  year={2021},
  booktitle={Proceedings of SIGIR},
  abbr={SIGIR},
  arxiv={2105.01736},
  code={https://github.com/FeiWang96/GTR},
  abstract={The task of natural language table retrieval (NLTR) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the NLTR problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents.}
}

@inproceedings{wu2020corefqa,
  title={CorefQA: Coreference resolution as query-based span prediction},
  author={Wu, Wei and Wang, Fei and Yuan, Arianna and Wu, Fei and Li, Jiwei},
  year={2020},
  booktitle={Proceedings of ACL},
  abbr={ACL},
  arxiv={1911.01746},
  code={https://github.com/ShannonAI/CorefQA},
  abstract={In this paper, we present an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in machine reading comprehension (MRC): A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the MRC framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing MRC datasets can be used for data augmentation to improve the model's generalization capability. Experiments demonstrate significant performance boost over previous models, with 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark.}
}

@inproceedings{meng2019glyce,
  title={Glyce: Glyph-vectors for chinese character representations},
  author={Meng*, Yuxian and Wu*, Wei and Wang*, Fei and Li*, Xiaoya and Nie, Ping and Yin, Fan and Li, Muyu and Han, Qinghong and Sun, Xiaofei and Li, Jiwei},
  year={2019},
  booktitle={Proceedings of NeurIPS},
  abbr={NeurIPS},
  arxiv={1901.10125},
  abstract={It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\% on the Fudan corpus for text classification.}
}
